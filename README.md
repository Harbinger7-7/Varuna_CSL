#  VARUNA – CSL Assistant

**Varuna** is a local Language Model (LLM)-based audit assistant built for regulatory evaluations in shipyard procurement, based on clause retrieval from manuals and rulebooks. It uses Mistral-7B with a context-aware prompt system and performs semantic search over official documentation using ChromaDB and SentenceTransformers.

---

##  Project Structure

```
LLM_Mistral_base/
│
├── app.py                  # Core logic: embedding, RAG, LLM, prompting
├── build.py                # PDF chunking, embedding, and index builder
├── UI.py                   # Gradio-based chatbot interface
├── Prompt_template.txt     # Compliance rules & prompt format
│
├── models/                 # GGUF LLM model (Mistral 7B Instruct)
│   └── mistral-7b-instruct-v0.2.Q4_K_M.gguf
│
├── Storage_m_varuna/       # Vector DB auto-generated by Chroma
├── data/                   # Rulebook PDF (e.g. Materials_Manual.pdf)
├── logo.gif                # Default avatar image
├── loading.gif             # Loading animation
```

---

##  Setup Instructions

### 1.  Install Dependencies

Use pip to install the required packages:

```bash
pip install llama-cpp-python sentence-transformers chromadb gradio PyMuPDF
```

> Optional: Save to a `requirements.txt` using:
> `pip freeze > requirements.txt`

---

### 2. ⬇ Download Model

Download the quantized model file:  
 [`mistral-7b-instruct-v0.2.Q4_K_M.gguf`](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF)

Place it in the following directory:
```
LLM_Mistral_base/models/
```

---

### 3.  Prepare the Rulebook

Save your reference manual as a PDF in the following directory:
```
LLM_Mistral_base/data/Materials_Manual.pdf
```

If your file has a different name or path, update `DEFAULT_PDF` in `build.py` accordingly.

---

### 4.  Build the Vector Index

This converts the PDF into searchable chunks and embeds them for semantic retrieval:

```bash
python build.py
```

This will create or replace the `Storage_m_varuna/` vector store.

---

### 5.  Launch the Application

Start the local assistant interface using:

```bash
python UI.py
```

A Gradio browser window will open where you can type queries like:

> _"Can we exceed the original PO quantity by 3x?"_  
> _"Is delivery extension allowed after one year?"_

---

##  Usage Notes

- The assistant only answers based on retrieved rule content.
- It does **not guess** or infer intent unless explicitly written in the clause.
- It follows RTI, CAG, and Vigilance audit logic as per prompt design.
- Answers follow the structure:  
  **Action Required**, **Procedure**, **Clause Justification**.

---

##  Developers

- Anand Raj
  B.Tech Artificial Intelligence and Data Science, 2025
  Rajagiri School of Engineering and Technology, Kochi

- Kestelyn Sunil Jacob
  B.Tech Artificial Intelligence and Data Science, 2025
  Rajagiri School of Engineering and Technology, Kochi

---

##  Compliance Domains Covered

- General Financial Rules (GFR)  
- GeM SOP (Government e-Marketplace)  
- Defence Procurement Procedure (DPP)  
- CSL Internal Procurement Manual  
- BIS/IS/ISO/ASTM Engineering Standards

---

##  License & Deployment

This tool is intended for **internal non-commercial audit and review purposes** only.

Deployment is offline and fully local — no internet/API calls are required.

---
## FUTURE SCOPE:

The UI and build files can be modularized to allow selection of department-specific manuals. Upon selection, the corresponding build file for that manual will be executed. This means either build.py and UI.py must be modified, or a new file can be introduced to handle the selection logic, passing the chosen manual to build.py and launching UI.py as usual.

The KBS (Knowledge-Based System: Varuna) can leverage the shipyard’s local network to significantly accelerate the process.

Running the system on local servers can further improve computing speed and allow deployment of more capable versions of Mistral. The current model is quantized due to GPU limitations on the laptop. For maximum efficiency, high-performance servers with advanced GPUs should be used.

Additionally, a dedicated application can be developed but only after the system is hosted online or on servers, given the high computational demands.
